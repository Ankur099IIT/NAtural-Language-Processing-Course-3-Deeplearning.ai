{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankur099IIT/Natural-Language-Processing-Course-3-Deeplearning.ai/blob/main/C3W1_Assignment_Explore_the_BBC_News_archive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "juvenile-enforcement",
      "metadata": {
        "id": "juvenile-enforcement"
      },
      "source": [
        "Explore the BBC News archive\n",
        "\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "combined-brooklyn",
      "metadata": {
        "id": "combined-brooklyn"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dependent-power",
      "metadata": {
        "id": "dependent-power"
      },
      "source": [
        "Begin by looking at the structure of the csv that contains the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "finite-panic",
      "metadata": {
        "id": "finite-panic"
      },
      "outputs": [],
      "source": [
        "with open(\"./bbc-text.csv\", 'r') as csvfile:\n",
        "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aggregate-calvin",
      "metadata": {
        "id": "aggregate-calvin"
      },
      "source": [
        "As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rocky-credit",
      "metadata": {
        "id": "rocky-credit"
      },
      "source": [
        "## Removing Stopwords\n",
        "\n",
        "One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n",
        "\n",
        "Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permanent-privilege",
      "metadata": {
        "id": "permanent-privilege"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: remove_stopwords\n",
        "def remove_stopwords(sentence):\n",
        "    \"\"\"\n",
        "    Removes a list of stopwords\n",
        "    \n",
        "    Args:\n",
        "        sentence (string): sentence to remove the stopwords from\n",
        "    \n",
        "    Returns:\n",
        "        sentence (string): lowercase sentence without the stopwords\n",
        "    \"\"\"\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "    \n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = sentence.lower()\n",
        "   \n",
        "   \n",
        "\n",
        "    sentencewords = sentence.split()\n",
        "\n",
        "    resultwords  = [word for word in sentencewords if word.lower() not in stopwords]\n",
        "    sentence = ' '.join(resultwords)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "   \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "northern-third",
      "metadata": {
        "id": "northern-third",
        "outputId": "1522a411-a7a8-452c-9c9e-8fbf1b9836b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go store get snack'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test your function\n",
        "remove_stopwords(\"I am about to go to the store and get any snack\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "monthly-beginning",
      "metadata": {
        "id": "monthly-beginning"
      },
      "outputs": [],
      "source": [
        "def parse_data_from_file(filename):\n",
        "    \"\"\"\n",
        "    Extracts sentences and labels from a CSV file\n",
        "    \n",
        "    Args:\n",
        "        filename (string): path to the CSV file\n",
        "    \n",
        "    Returns:\n",
        "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        ### START CODE HERE\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            labels.append(row[0])\n",
        "            sentence = row[1]\n",
        "            sentence = remove_stopwords(sentence)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "listed-saturn",
      "metadata": {
        "id": "listed-saturn",
        "outputId": "60d21756-eb86-4fcd-d94f-661749daf5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2225 sentences in the dataset.\n",
            "\n",
            "First sentence has 436 words (after removing stopwords).\n",
            "\n",
            "There are 2225 labels in the dataset.\n",
            "\n",
            "The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n"
          ]
        }
      ],
      "source": [
        "# Test your function\n",
        "sentences, labels = parse_data_from_file(\"./bbc-text.csv\")\n",
        "\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 5 labels are {labels[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cultural-virginia",
      "metadata": {
        "id": "cultural-virginia"
      },
      "outputs": [],
      "source": [
        "def fit_tokenizer(sentences):\n",
        "    \"\"\"\n",
        "    Instantiates the Tokenizer class\n",
        "    \n",
        "    Args:\n",
        "        sentences (list): lower-cased sentences without stopwords\n",
        "    \n",
        "    Returns:\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    # Instantiate the Tokenizer class by passing in the oov_token argument\n",
        "    tokenizer = Tokenizer(oov_token = '<OOV>')\n",
        "    # Fit on the sentences\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    ### END CODE HERE\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tracked-hostel",
      "metadata": {
        "id": "tracked-hostel",
        "outputId": "3370b23e-72b8-4827-fbcf-720f69dde24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary contains 29714 words\n",
            "\n",
            "<OOV> token included in vocabulary\n"
          ]
        }
      ],
      "source": [
        "tokenizer = fit_tokenizer(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "golden-flash",
      "metadata": {
        "id": "golden-flash"
      },
      "outputs": [],
      "source": [
        "def get_padded_sequences(tokenizer, sentences):\n",
        "    \"\"\"\n",
        "    Generates an array of token sequences and pads them to the same length\n",
        "    \n",
        "    Args:\n",
        "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
        "        sentences (list of string): list of sentences to tokenize and pad\n",
        "    \n",
        "    Returns:\n",
        "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # Pad the sequences using the post padding strategy\n",
        "    padded_sequences = pad_sequences(sequences, padding = 'post')\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spanish-entrepreneur",
      "metadata": {
        "id": "spanish-entrepreneur",
        "outputId": "61cedf25-92f9-4b7d-e9f1-e7dbc47f94bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First padded sequence looks like this: \n",
            "\n",
            "[  96  176 1157 ...    0    0    0]\n",
            "\n",
            "Numpy array of all sequences has shape: (2225, 2438)\n",
            "\n",
            "This means there are 2225 sequences in total and each one has a size of 2438\n"
          ]
        }
      ],
      "source": [
        "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
        "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
        "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
        "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unknown-optimization",
      "metadata": {
        "id": "unknown-optimization"
      },
      "outputs": [],
      "source": [
        "def tokenize_labels(labels):\n",
        "    \"\"\"\n",
        "    Tokenizes the labels\n",
        "    \n",
        "    Args:\n",
        "        labels (list of string): labels to tokenize\n",
        "    \n",
        "    Returns:\n",
        "        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Instantiate the Tokenizer class\n",
        "    # No need to pass additional arguments since you will be tokenizing the labels\n",
        "    label_tokenizer = Tokenizer()\n",
        "    \n",
        "    # Fit the tokenizer to the labels\n",
        "    label_tokenizer.fit_on_texts(labels)\n",
        "    \n",
        "    # Save the word index\n",
        "    label_word_index = label_tokenizer.word_index\n",
        "    \n",
        "    # Save the sequences\n",
        "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
        "\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return label_sequences, label_word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "streaming-conviction",
      "metadata": {
        "id": "streaming-conviction",
        "outputId": "6f82c4c1-8386-40b0-9172-8d76e805a3cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "\n",
            "First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "label_sequences, label_word_index = tokenize_labels(labels)\n",
        "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
        "print(f\"First ten sequences {label_sequences[:10]}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "C3W1_Assignment-Explore the BBC News archive.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}